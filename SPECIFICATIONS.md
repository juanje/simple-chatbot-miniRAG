# Simple Chatbot - Technical Specifications

## ğŸ“‹ Project Overview

**Simple Chatbot** is a command-line application that implements a conversational chatbot using LangChain and Ollama. The project is designed for educational purposes and to demonstrate best practices in developing modern Python applications with local LLMs.

## ğŸ¯ Objectives

- **Educational**: Demonstrate chatbot implementation using modern technologies
- **Modular**: Clean architecture with separation of concerns
- **User-friendly**: Intuitive CLI interface with special commands
- **Configurable**: Adjustable parameters for different use cases
- **Robust**: Complete error handling and validations

## ğŸ—ï¸ Architecture

### Project Structure

```
simple-chatbot/
â”œâ”€â”€ src/simple_chatbot/
â”‚   â”œâ”€â”€ __init__.py          # Main module
â”‚   â”œâ”€â”€ chatbot.py           # Core chatbot logic
â”‚   â”œâ”€â”€ cli.py               # Command-line interface
â”‚   â”œâ”€â”€ config.py            # Centralized configuration
â”‚   â”œâ”€â”€ llm_client.py        # Ollama client
â”‚   â””â”€â”€ memory.py            # Conversational memory management
â”œâ”€â”€ tests/                   # Unit tests
â”œâ”€â”€ pyproject.toml          # Project configuration
â”œâ”€â”€ README.md               # User documentation
â””â”€â”€ SPECIFICATIONS.md       # This file
```

### Main Components

#### 1. **SimpleChatbot** (`chatbot.py`)
- **Responsibility**: Conversation orchestration
- **Features**:
  - Conversation flow management
  - Integration with memory and LLM client
  - Prompt formatting
  - Conversation statistics
  - Health checks

#### 2. **CLI Interface** (`cli.py`)
- **Responsibility**: User interface
- **Features**:
  - Rich interface with panels and colors
  - Command history navigation (â†‘â†“)
  - Special commands with `/` prefix
  - Keyboard shortcuts (Ctrl+L to clear)
  - User-friendly error handling

#### 3. **OllamaClient** (`llm_client.py`)
- **Responsibility**: Communication with Ollama
- **Features**:
  - Ollama connection via HTTP
  - Available model validation
  - Generation parameter configuration
  - Connection error handling

#### 4. **ConversationMemory** (`memory.py`)
- **Responsibility**: Conversational memory management
- **Features**:
  - Configurable message limit
  - Prompt formatting
  - Conversation statistics
  - History reset

#### 5. **ChatbotConfig** (`config.py`)
- **Responsibility**: Centralized configuration
- **Features**:
  - Model parameters (temperature, max_tokens)
  - Connection URLs
  - Memory limits
  - System prompt

## ğŸ› ï¸ Technology Stack

### Core Dependencies
- **Python**: 3.10+ (modern type hints)
- **LangChain**: Framework for LLM applications
- **LangChain-Ollama**: Specific Ollama integration
- **Pydantic**: Data validation and configuration

### CLI & UX
- **Rich**: Advanced terminal interface
- **Click**: CLI framework
- **prompt-toolkit**: Advanced input with history

### Development Tools
- **uv**: Dependency and virtual environment management
- **ruff**: Code linting and formatting
- **pytest**: Testing framework
- **mypy**: Static type checking

## ğŸ“‹ Implemented Features

### Core Features
- âœ… **Basic conversation**: Interactive chat with local LLMs
- âœ… **Conversational memory**: Maintains conversation context
- âœ… **Flexible configuration**: Multiple adjustable parameters
- âœ… **Multiple models**: Support for any Ollama model

### Special Commands
- âœ… `/quit`, `/exit`, `/bye`: End conversation
- âœ… `/reset`: Clear conversation history
- âœ… `/stats`: Show conversation statistics
- âœ… `/history`: View complete history
- âœ… `/help`: Show help

### UX Features
- âœ… **Rich Interface**: Colored panels and improved formatting
- âœ… **Command history**: â†‘â†“ navigation
- âœ… **Shortcuts**: Ctrl+L to clear screen
- âœ… **Loading indicators**: Spinners during processing
- âœ… **Error handling**: Informative messages

### CLI Options
- âœ… `--model`: Ollama model selection
- âœ… `--temperature`: Creativity control (0.0-1.0)
- âœ… `--max-tokens`: Response token limit
- âœ… `--long-responses`: Long response mode (4000 tokens)
- âœ… `--ollama-url`: Custom Ollama URL
- âœ… `--memory-limit`: Message limit in memory
- âœ… `--debug`: Detailed logging

## ğŸ”§ Configuration

### Environment Variables
```bash
OLLAMA_BASE_URL=http://localhost:11434
MODEL_NAME=llama2
TEMPERATURE=0.7
MAX_TOKENS=2000
CONVERSATION_MEMORY_LIMIT=10
```

### Programmatic Configuration
```python
config = ChatbotConfig(
    ollama_base_url="http://localhost:11434",
    model_name="mistral",
    temperature=0.5,
    max_tokens=3000,
    conversation_memory_limit=15
)
```

## ğŸš€ Installation and Usage

### Local Installation
```bash
# Clone and enter directory
git clone <repo-url>
cd simple-chatbot

# Install dependencies
uv sync

# Verify installation
uv run chatbot --help
```

### Global Installation
```bash
# Option 1: pipx (recommended)
pipx install .

# Option 2: uv tool
uv tool install .
```

### Basic Usage
```bash
# Run with default configuration
uv run chatbot

# Use specific model
uv run chatbot --model mistral --temperature 0.5

# Debug mode for development
uv run chatbot --debug
```

## ğŸ§ª Testing

### Test Structure
```
tests/
â”œâ”€â”€ test_chatbot.py     # Main component tests
â”œâ”€â”€ test_config.py      # Configuration tests
â””â”€â”€ test_memory.py      # Conversational memory tests
```

### Running Tests
```bash
# Complete tests
uv run pytest

# With coverage
uv run pytest --cov=src/simple_chatbot
```

## ğŸ“Š Metrics and Statistics

The chatbot provides the following metrics:
- **Total messages**: Total number of exchanged messages
- **User messages**: User messages
- **Bot messages**: Bot responses
- **Average message length**: Average message length
- **Conversation duration**: Session duration

## ğŸ” Security and Considerations

### Security
- **Local data**: All data remains on the local system
- **No telemetry**: No data sent to external services
- **Input validation**: Basic input sanitization

### Limitations
- **Ollama dependency**: Requires Ollama running locally
- **Volatile memory**: History is lost when closing the application
- **No persistence**: No permanent conversation storage

## ğŸ“ˆ Possible Future Improvements

### Features
- [ ] **Persistence**: Save conversations to database
- [ ] **Multiple sessions**: Manage multiple conversations
- [ ] **Export/Import**: Export conversations to files
- [ ] **File configuration**: TOML/YAML configuration files
- [ ] **Plugins**: Extension system

### Technical
- [ ] **Async/await**: Improve concurrency
- [ ] **Streaming**: Real-time responses
- [ ] **Rate limiting**: Usage limits
- [ ] **Metrics**: Advanced metrics with Prometheus
- [ ] **Web UI**: Optional web interface

## ğŸ¤ Contributions

### Code Standards
- **Type hints**: Mandatory in all functions
- **Docstrings**: Google style for all public functions
- **Ruff**: Automatic linting and formatting
- **Tests**: Minimum 90% coverage

### Development Process
1. **Fork** the repository
2. **Branch** for new feature
3. **Tests** for any change
4. **PR** with detailed description

## ğŸ“ Versioning

The project follows [Semantic Versioning](https://semver.org/):
- **MAJOR**: Incompatible API changes
- **MINOR**: Compatible new features
- **PATCH**: Compatible bug fixes

**Current version**: 0.1.0

---

## ğŸ“„ License

This project is designed for educational and demonstration purposes.

**Author**: Juanje Ojeda (juanje@redhat.com)
**Date**: July 2025